{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Transformers, BERT and Transfer Learning with Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Subword tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Compare the tokenizations of the mBERT tokenizer of texts from two different language(-varietie)s you are able to understand/read. Use English, the dominant language in mBERT, with a lower-resource language variety (for example Danish). If you only know 1 language, try to use a different variety of the language (for example for English, use social media abbreviations or typos, e.g.: c u tmrw). You can collect data from any source, or make up your own sentences.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asgerkromand/miniconda3/envs/deep/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'Andreas', ',', 'and', 'I', 'am', 'a', 'guest', 'student', 'at', 'ITU']\n",
      "['Mit', 'navn', 'er', 'Andreas', ',', 'og', 'jeg', 'er', 'en', 'g', '##æste', '##stu', '##deren', '##de', 'ved', 'ITU']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asgerkromand/miniconda3/envs/deep/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "english_text = \"My name is Andreas, and I am a guest student at ITU\"\n",
    "danish_text = \"Mit navn er Andreas, og jeg er en gæstestuderende ved ITU\"\n",
    "\n",
    "print(tokenizer.tokenize(english_text))\n",
    "print(tokenizer.tokenize(danish_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Now test the tokenizer of a language model that is trained for your target language. You can find language models on https://huggingface.co/search/full-text?type=model. Can you observe any differences in the results (in amount/length of subwords)? Do the results match your intuition of separating mostly short meaning-carrying subwords?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'name', 'is', 'andreas', ',', 'and', 'i', 'am', 'a', 'gu', '##est', 'student', 'at', 'it', '##u']\n",
      "['mit', 'navn', 'er', 'andreas', ',', 'og', 'jeg', 'er', 'en', 'gæste', '##studerende', 'ved', 'it', '##u']\n"
     ]
    }
   ],
   "source": [
    "danish_tokenizer = AutoTokenizer.from_pretrained(\"Maltehb/danish-bert-botxo\")\n",
    "\n",
    "print(danish_tokenizer.tokenize(english_text)) # it doesn't work here ofc\n",
    "print(danish_tokenizer.tokenize(danish_text)) # but now it works here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Think of two example inputs where the tokenizer might struggle to find a meaningful segmentation (for example by introducing typos). Why are these cases difficult?, did the tokenizer do something sensible?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'students', 'aren', '##lt', 'dor', '##ing', 'a', 'good', 'job', 'with', 'the', 'ai', 'reso', '##urs', '##es', 'they', 'have']\n",
      "['de', 'studer', '##nd', '##n', '##fe', 'gør', 'det', 'ringe', 'med', 'den', 'k', '##su', '##ns', '##tige', 'intel', '##igen', '##s', 'de', 'har']\n"
     ]
    }
   ],
   "source": [
    "# try examples with spelling errors\n",
    "bad_english_text = \"The students arenlt doring a good  job with the ai resourses they have\"\n",
    "bad_danish_text = \"De studerndnfe gør det ringe med den ksunstige inteligens de har\"\n",
    "\n",
    "print(tokenizer.tokenize(bad_english_text))\n",
    "print(danish_tokenizer.tokenize(bad_danish_text))\n",
    "\n",
    "# works bad but still catches a lot of meaningful subwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cross-domain transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Train a sentiment analysis model with BERT (bert-base-cased) on the English SST data. Evaluate it on the SST data as well as on the English Twitter data from SemEval2013. Is there a similar performance drop as in assignment 1?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n",
      "tokenizing...\n",
      "tokenizer_config.json: 100%|█████████████████| 49.0/49.0 [00:00<00:00, 63.1kB/s]\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "config.json: 100%|█████████████████████████████| 570/570 [00:00<00:00, 3.02MB/s]\n",
      "vocab.txt: 100%|█████████████████████████████| 213k/213k [00:00<00:00, 1.16MB/s]\n",
      "tokenizer.json: 100%|████████████████████████| 436k/436k [00:00<00:00, 4.21MB/s]\n",
      "converting to batches...\n",
      "initializing model...\n",
      "model.safetensors: 100%|█████████████████████| 436M/436M [00:48<00:00, 9.00MB/s]\n",
      "training...\n",
      "=====================\n",
      "starting epoch 0\n",
      "Loss: 3969.92\n",
      "Acc(dev): 89.24\n",
      "\n",
      "=====================\n",
      "starting epoch 1\n",
      "Loss: 2006.38\n",
      "Acc(dev): 90.74\n",
      "\n",
      "=====================\n",
      "starting epoch 2\n",
      "Loss: 1250.86\n",
      "Acc(dev): 89.70\n",
      "\n",
      "=====================\n",
      "starting epoch 3\n",
      "Loss: 776.82\n",
      "Acc(dev): 91.20\n",
      "\n",
      "=====================\n",
      "starting epoch 4\n",
      "Loss: 501.08\n",
      "Acc(dev): 91.20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 sentiment/bert-classification.py sentiment/sst.train sentiment/sst.dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n",
      "tokenizing...\n",
      "converting to batches...\n",
      "initializing model...\n",
      "training...\n",
      "=====================\n",
      "starting epoch 0\n",
      "Loss: 3969.92\n",
      "Acc(dev): 87.10\n",
      "\n",
      "=====================\n",
      "starting epoch 1\n",
      "Loss: 2006.38\n",
      "Acc(dev): 87.10\n",
      "\n",
      "=====================\n",
      "starting epoch 2\n",
      "Loss: 1250.86\n",
      "Acc(dev): 86.70\n",
      "\n",
      "=====================\n",
      "starting epoch 3\n",
      "Loss: 776.82\n",
      "Acc(dev): 85.37\n",
      "\n",
      "=====================\n",
      "starting epoch 4\n",
      "Loss: 501.08\n",
      "Acc(dev): 84.31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 sentiment/bert-classification.py sentiment/sst.train sentiment/semeval2013.dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final accuracies drop from 91.20% to 84.31% (sst.dev vs. semeval2013.dev).\n",
    "\n",
    "In assignment 1, the accuracy drop was from 77.18% to 64.74% (sst.dev vs. semeval2013.dev)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Inspect the code and try to understand the steps of the inference and the training procedure. What is the shape of the output scores variable of the forward function?, what do the dimensions represent?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Dimensions:*\n",
    "- The shape of the output_scores variable is (batch_size, num_labels).\n",
    "- batch_size is the number of sentences in the batch (set to 16 in the code).\n",
    "- num_labels is the number of classes in the dataset (in the code it's set to the length of list of unique labels).\n",
    "\n",
    "*Inference:*\n",
    "1. BERT gets tensors of wordpiece indices and a mask (both of shape (batch_size, max_sent_len)) as input, then producing hidden states.\n",
    "2. We extract [CLS] tokens, which works as a sort of sentence summary.\n",
    "3. A linear layer transforms the CLS representation into a vector of output scores for each label, which represents the probability of the sentence being of a certain label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Now train a sentiment model with the twitter embeddings (cardiffnlp/twitter-xlm-roberta-base) with the SST train data. Does it transfer better to the English Twitter data compared to the mBERT model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n",
      "tokenizing...\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "config.json: 100%|█████████████████████████████| 652/652 [00:00<00:00, 1.36MB/s]\n",
      "sentencepiece.bpe.model: 100%|█████████████| 5.07M/5.07M [00:00<00:00, 10.2MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 9.10M/9.10M [00:01<00:00, 7.09MB/s]\n",
      "converting to batches...\n",
      "initializing model...\n",
      "pytorch_model.bin: 100%|███████████████████| 1.11G/1.11G [01:53<00:00, 9.82MB/s]\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "training...\n",
      "=====================\n",
      "starting epoch 0\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/andreasalkemade/Desktop/NLP/assignment2/sentiment/bert-classification2.py\", line 170, in <module>\n",
      "    optimizer.step()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/optimizer.py\", line 391, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/optimizer.py\", line 76, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/adam.py\", line 168, in step\n",
      "    adam(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/adam.py\", line 318, in adam\n",
      "    func(params,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/adam.py\", line 441, in _single_tensor_adam\n",
      "    denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)\n",
      "             ~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n",
      "RuntimeError: MPS backend out of memory (MPS allocated: 5.56 GB, other allocations: 12.20 GB, max allowed: 18.13 GB). Tried to allocate 732.43 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    }
   ],
   "source": [
    "!python3 sentiment/bert-classification2.py sentiment/sst.train sentiment/semeval2013.dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not possible to run (due to lack of memory, I think)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cross-lingual transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Train an English BERT model (bert-base-cased on huggingface) on the reviews data from SST, and evaluate it on the SST data as well as the Danish Twitter data. Is there a performance drop when going to the Danish data? How does the performance on the Danish data compare to the majority baseline?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n",
      "tokenizing...\n",
      "converting to batches...\n",
      "initializing model...\n",
      "training...\n",
      "=====================\n",
      "starting epoch 0\n",
      "Loss: 3969.92\n",
      "\n",
      "=====================\n",
      "starting epoch 1\n",
      "Loss: 2006.38\n",
      "\n",
      "=====================\n",
      "starting epoch 2\n",
      "Loss: 1250.86\n",
      "\n",
      "=====================\n",
      "starting epoch 3\n",
      "Loss: 776.82\n",
      "\n",
      "=====================\n",
      "starting epoch 4\n",
      "Loss: 501.08\n",
      "\n",
      "Model saved to sentiment_model.pth\n"
     ]
    }
   ],
   "source": [
    "!python3 sentiment/bert-classification3.py train sentiment/sst.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading test data...\n",
      "tokenizing...\n",
      "converting to batches...\n",
      "Acc(test): 91.20\n"
     ]
    }
   ],
   "source": [
    "!python3 sentiment/bert-classification3.py evaluate sentiment/sst.dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading test data...\n",
      "tokenizing...\n",
      "converting to batches...\n",
      "Acc(test): 63.59\n"
     ]
    }
   ],
   "source": [
    "!python3 sentiment/bert-classification3.py evaluate sentiment/twitter-da.dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big drop in performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Train an mBERT model (bert-base-multilingual-cased on huggingface) model on the reviews data from SST, and evaluate it on the Danish development split, what is the performance? How does it compare to the English BERT model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Autoregressive language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the code in qa.py as a starting point. For evaluation, it checks whether should be noted that\n",
    "the evaluation metric is a custom metric “designed” by Rob, it checks whether at least half of the gold words\n",
    "in the predicted output.\n",
    "1. Add 5 common-knowledge questions and answers in the corresponding python lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used these questions and answers for this task:\n",
    "\n",
    "questions = [\n",
    "\"What is the most populated country in the world?\",\n",
    "\"What is a boy to his mom?\",\n",
    "\"Which country lost second world war?\",\n",
    "\"What city is called 'The big apple'?\",\n",
    "\"What country was Chistopher Columbus looking for when he discovered America?\"]\n",
    "\n",
    "answers = [\n",
    "\"India.\",\n",
    "\"Her son.\",\n",
    "\"Germany.\",\n",
    "\"New York.\",\n",
    "\"India.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) What is the performance of FLAN-t5 base on your questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/Users/asgerkromand/miniconda3/envs/deep/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      " What is the most populated country in the world? \n",
      "sweden\n",
      "\n",
      " What is a boy to his mom? \n",
      "a sailor\n",
      "\n",
      " Which country lost second world war? \n",
      "poland\n",
      "\n",
      " What city is called 'The big apple'? \n",
      "san francisco\n",
      "\n",
      " What country was Chistopher Columbus looking for when he discovered America? \n",
      "el reino de espaa\n",
      "\n",
      "0 out of 5 correct\n"
     ]
    }
   ],
   "source": [
    "!python3 qa/qa.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It answered 5 out 5 incorrectly. Bad performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What are possible pitfals of the evaluation metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very binary. There is some information loss. Furthermore, some answers might induce words that are just more likely by random, and this is not evaluated through the metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. If the model has made some errors: why is this the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't seem to understand which words are the important. E.g. the last question, where it knows it is something about a ship, but it doesn't recognise that it is specifically Bobba Fett's ship. The correct answer was not probable enough, maybe because Boba Fett is a rare word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Experiment with at least 2 different prefixes and postfixes; do they improve performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/Users/asgerkromand/miniconda3/envs/deep/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "The following is a question you need to answer: What is the most populated country in the world? The answer to that question is..\n",
      "China\n",
      "\n",
      "The following is a question you need to answer: What language do they speak in Brazil? The answer to that question is..\n",
      "Portuguese Language\n",
      "\n",
      "The following is a question you need to answer: Which country lost second world war? The answer to that question is..\n",
      "poland\n",
      "\n",
      "The following is a question you need to answer: What city is called 'The big apple'? The answer to that question is..\n",
      "san francisco\n",
      "\n",
      "The following is a question you need to answer: What country was Chistopher Columbus looking for when he discovered America? The answer to that question is..\n",
      "United States of America\n",
      "\n",
      "1 out of 5 correct\n",
      "I have a simple question for you: What is the most populated country in the world? The short answer to that is:\n",
      "China\n",
      "\n",
      "I have a simple question for you: What language do they speak in Brazil? The short answer to that is:\n",
      "Brazilian Portuguese\n",
      "\n",
      "I have a simple question for you: Which country lost second world war? The short answer to that is:\n",
      "Germany\n",
      "\n",
      "I have a simple question for you: What city is called 'The big apple'? The short answer to that is:\n",
      "New York City\n",
      "\n",
      "I have a simple question for you: What country was Chistopher Columbus looking for when he discovered America? The short answer to that is:\n",
      "United States of America\n",
      "\n",
      "3 out of 5 correct\n"
     ]
    }
   ],
   "source": [
    "# First set of prefix and postfix\n",
    "\n",
    "!python3 qa/qa.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is better for both sets - especially for my last set of prefix and postfix. The difference could be that I added the adjectives \"simple\" and \"short\". It might make it work for an more intuitive answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Question answering with FLAN-T5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will evaluate the FLAN-T5 model on the Star Wars domain. For this, I have scraped 66 Star Wars\n",
    "trivia questions from https://parade.com/1161189/alexandra-hurtado/star-wars-trivia/ they have\n",
    "been pre-processed and are available in questions.txt and answers.txt.\n",
    "1. What is the performance of the FLAN-T5 model out-of-the-box?\n",
    "2. Can you improve performance with your pre- and post- fixes? Why?\n",
    "3. We also provide you with the raw text from Wookieepedia, this is a fandom wiki with information about the Star Wars universe written in English. It has been scraped using the procedure described on https://robvanderg.github.io/datasets/wikia/, and is available in starwarsfandomcom-20200223.txt.cleaned.tok.uniq.txt.gz. Use the words from the questions to find the 5 sentences with the highest word overlap (the raw sentences with the largest coverage of words with respect to the question). Add these sentences as a prefix, separated with newlines. Does performance increase?\n",
    "4. Bonus: experiment with better variants of data selection, what is the highest score you can obtain?\n",
    "Note that ChatGPT achieved a score of 51; feel free to also use larger language models, for example\n",
    "google/flan-t5-large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the performance of the FLAN-T5 model out-of-the-box?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/Users/asgerkromand/miniconda3/envs/deep/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      " Where did Obi-Wan take Luke after his birth? \n",
      "san diego\n",
      "\n",
      " Who is Palpatine's granddaughter? \n",
      "elizabeth\n",
      "\n",
      " Who was Anakin Skywalker's Padawan? \n",
      "taiwan\n",
      "\n",
      " Where is Jabba the Hutt's Palace located? \n",
      "san francisco\n",
      "\n",
      " What's the name of Boba Fett's ship? \n",
      "sailor s sailor\n",
      "\n",
      " Who are Kylo Ren's parents? \n",
      "evan ron\n",
      "\n",
      " Who killed Qui-Gon Jinn? \n",
      "taekwondo player\n",
      "\n",
      " According to Yoda, there are always how many Sith Lords...no more, no less? \n",
      "ten\n",
      "\n",
      " Who built C-3PO? \n",
      "samuel h. savage\n",
      "\n",
      " What is the name of Han Solo's ship? \n",
      "sailor of saigon\n",
      "\n",
      " Who acted as Queen Amidala's decoy? \n",
      "samuel harrison\n",
      "\n",
      " What was Finn's stormtrooper number? \n",
      "ten\n",
      "\n",
      " Where was Yoda's home in his final years? \n",
      "san juan\n",
      "\n",
      " Chancellor Palpatine was which Sith Lord? \n",
      "charles ii\n",
      "\n",
      " Han Solo was frozen in what? \n",
      "ice\n",
      "\n",
      " Who had Rey's parents taken and killed? \n",
      "samuel samuel\n",
      "\n",
      " Luke lost which of his hands in a fight with Darth Vader? \n",
      "ring finger\n",
      "\n",
      " Anakin Skywalker grew up to be who? \n",
      "a samurai\n",
      "\n",
      " Per Yoda, what is the path to the dark side? \n",
      "samurai\n",
      "\n",
      " What species is Chewbacca? \n",
      "genus of lizards\n",
      "\n",
      " Padmé was Queen of what? \n",
      "scotland\n",
      "\n",
      " What is Kylo Ren's birth name? \n",
      "kylo ren\n",
      "\n",
      " Who is Luke and Leia's father? \n",
      "john dillinger\n",
      "\n",
      " Who told Anakin there is another Skywalker? \n",
      "daniel savage\n",
      "\n",
      " Leia made who acting general? \n",
      "john d. harris\n",
      "\n",
      " Which character said, \"Help me, Obi-Wan Kenobi. You are my only hope\"? \n",
      "obi-wan kenobi\n",
      "\n",
      " Where was Baby Yoda when Order 66 was initiated? \n",
      "saigon\n",
      "\n",
      " What color was Mace Windu's lightsaber? \n",
      "blue\n",
      "\n",
      " What color was Yoda's lightsaber? \n",
      "blue\n",
      "\n",
      " C-3PO is fluent in over how many forms of communication? \n",
      "over 100\n",
      "\n",
      " What species is Jar Jar Binks? \n",
      "a genus of flies\n",
      "\n",
      " What is the name of Anakin's stepbrother? \n",
      "samurai\n",
      "\n",
      " Who was Boba Fett's father? \n",
      "daniel drax\n",
      "\n",
      " Who killed Mace Windu? \n",
      "samuel savage\n",
      "\n",
      " What is the name of the female member of the Jedi High Council who is of the same species as Yoda? \n",
      "shira\n",
      "\n",
      " What was Galen Erso's nickname for his daughter? \n",
      "sailor\n",
      "\n",
      " True or False, Boba Fett survived the Sarlacc pit \n",
      "True: Boba Fett survived the Sarlacc pit. False: Boba Fett survived\n",
      "\n",
      " How many episodes are in the Skywalker Saga? \n",
      "10\n",
      "\n",
      " Who is known for saying the line: \"I have spoken\"? \n",
      "john dillinger\n",
      "\n",
      " Who played Princess Leia? \n",
      "emma streep\n",
      "\n",
      " Which actor played Luke Skywalker? \n",
      "joe mccartney\n",
      "\n",
      " Who does Obi-Wan say was the \"Chosen One\"? \n",
      "tao joo\n",
      "\n",
      " Who does BB-8 belong to? \n",
      "the United States\n",
      "\n",
      " How many children does Darth Vader have? \n",
      "three\n",
      "\n",
      " Lightsabers are powered by what type of crystal? \n",
      "a halogen\n",
      "\n",
      " Which actor played Lando Calrissian? \n",
      "john dillinger\n",
      "\n",
      " Where do Rey and BB-8 first meet? \n",
      "a secluded mountain village\n",
      "\n",
      " Who said, “I know what I have to do, but I don't know that I have the strength to do it”? \n",
      "john dillinger\n",
      "\n",
      " According to Luke, confronting what is the destiny of a Jedi? \n",
      "a rebirth\n",
      "\n",
      " Who is Darth Vader's grandson? \n",
      "daniel drax\n",
      "\n",
      " Leia said never underestimate a what? \n",
      "sailor\n",
      "\n",
      " X-wing fighters were used by the Resistance or the First Order? \n",
      "the First Order\n",
      "\n",
      " Jyn Erso said rebellions are built on what? \n",
      "sand\n",
      "\n",
      " Legend describes what as the hidden world of the Sith? \n",
      "saigon\n",
      "\n",
      " Who killed Snoke? \n",
      "a tiger\n",
      "\n",
      " What was Mando's weapon against Moff Gideon's darksaber? \n",
      "saber\n",
      "\n",
      " Who is the black market droidsmith Poe knows on Kijimi? \n",
      "a droid named samurai\n",
      "\n",
      " What compasses lead the way to Exegol? \n",
      "sandstone\n",
      "\n",
      " What was Poe Dameron's old job before becoming a pilot? \n",
      "scout\n",
      "\n",
      " Luke said, \"Confronting ___ is the destiny of a Jedi\"? \n",
      "adolescent\n",
      "\n",
      " Who killed Han Solo? \n",
      "samurai\n",
      "\n",
      " C-3PO is mechanically incapable of speaking translations from who? \n",
      "russia\n",
      "\n",
      " According to Zorii, what can give free passage through any blockade and landing privileges, any vessel? \n",
      "a sailor\n",
      "\n",
      " Who famously said, \"These aren't the droids you're looking for\"? \n",
      "john dillinger\n",
      "\n",
      " Rey and Kylo are a what ___ in the Force? \n",
      "scouts\n",
      "\n",
      " Who wins by making people think they are alone? \n",
      "a sailor\n",
      "\n",
      "0 out of 66 correct\n"
     ]
    }
   ],
   "source": [
    "# Pass the questions and answers to the QA model\n",
    "!python3 qa/qa.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Can you improve performance with your pre- and post- fixes? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/Users/asgerkromand/miniconda3/envs/deep/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "The following question is about Star Wars: Where did Obi-Wan take Luke after his birth? The answer to the question is:\n",
      "the Star Wars galaxy\n",
      "\n",
      "The following question is about Star Wars: Who is Palpatine's granddaughter? The answer to the question is:\n",
      "Princess Leia\n",
      "\n",
      "The following question is about Star Wars: Who was Anakin Skywalker's Padawan? The answer to the question is:\n",
      "Padawan\n",
      "\n",
      "The following question is about Star Wars: Where is Jabba the Hutt's Palace located? The answer to the question is:\n",
      "in the middle of the Star Wars galaxy\n",
      "\n",
      "The following question is about Star Wars: What's the name of Boba Fett's ship? The answer to the question is:\n",
      "Star Wars: The Force Awakens\n",
      "\n",
      "The following question is about Star Wars: Who are Kylo Ren's parents? The answer to the question is:\n",
      "Kylo Ren\n",
      "\n",
      "The following question is about Star Wars: Who killed Qui-Gon Jinn? The answer to the question is:\n",
      "Han Solo\n",
      "\n",
      "The following question is about Star Wars: According to Yoda, there are always how many Sith Lords...no more, no less? The answer to the question is:\n",
      "ten\n",
      "\n",
      "The following question is about Star Wars: Who built C-3PO? The answer to the question is:\n",
      "Lucasfilm\n",
      "\n",
      "The following question is about Star Wars: What is the name of Han Solo's ship? The answer to the question is:\n",
      "Star Wars: The Last Jedi\n",
      "\n",
      "The following question is about Star Wars: Who acted as Queen Amidala's decoy? The answer to the question is:\n",
      "Luke Skywalker\n",
      "\n",
      "The following question is about Star Wars: What was Finn's stormtrooper number? The answer to the question is:\n",
      "X-Wing\n",
      "\n",
      "The following question is about Star Wars: Where was Yoda's home in his final years? The answer to the question is:\n",
      "Yokohama\n",
      "\n",
      "The following question is about Star Wars: Chancellor Palpatine was which Sith Lord? The answer to the question is:\n",
      "Yves Padmé Amidala\n",
      "\n",
      "The following question is about Star Wars: Han Solo was frozen in what? The answer to the question is:\n",
      "ice\n",
      "\n",
      "The following question is about Star Wars: Who had Rey's parents taken and killed? The answer to the question is:\n",
      "The Empire\n",
      "\n",
      "The following question is about Star Wars: Luke lost which of his hands in a fight with Darth Vader? The answer to the question is:\n",
      "a right hand\n",
      "\n",
      "The following question is about Star Wars: Anakin Skywalker grew up to be who? The answer to the question is:\n",
      "Jedi\n",
      "\n",
      "The following question is about Star Wars: Per Yoda, what is the path to the dark side? The answer to the question is:\n",
      "a saber\n",
      "\n",
      "The following question is about Star Wars: What species is Chewbacca? The answer to the question is:\n",
      "a genus of ants\n",
      "\n",
      "The following question is about Star Wars: Padmé was Queen of what? The answer to the question is:\n",
      "the Empire\n",
      "\n",
      "The following question is about Star Wars: What is Kylo Ren's birth name? The answer to the question is:\n",
      "Kylo Ren\n",
      "\n",
      "The following question is about Star Wars: Who is Luke and Leia's father? The answer to the question is:\n",
      "Luke Skywalker\n",
      "\n",
      "The following question is about Star Wars: Who told Anakin there is another Skywalker? The answer to the question is:\n",
      "Luke Skywalker\n",
      "\n",
      "The following question is about Star Wars: Leia made who acting general? The answer to the question is:\n",
      "Luke Skywalker\n",
      "\n",
      "The following question is about Star Wars: Which character said, \"Help me, Obi-Wan Kenobi. You are my only hope\"? The answer to the question is:\n",
      "Obi-Wan Kenobi\n",
      "\n",
      "The following question is about Star Wars: Where was Baby Yoda when Order 66 was initiated? The answer to the question is:\n",
      "the Star Wars Episode I\n",
      "\n",
      "The following question is about Star Wars: What color was Mace Windu's lightsaber? The answer to the question is:\n",
      "blue\n",
      "\n",
      "The following question is about Star Wars: What color was Yoda's lightsaber? The answer to the question is:\n",
      "blue\n",
      "\n",
      "The following question is about Star Wars: C-3PO is fluent in over how many forms of communication? The answer to the question is:\n",
      "over a hundred\n",
      "\n",
      "The following question is about Star Wars: What species is Jar Jar Binks? The answer to the question is:\n",
      "a tadpole\n",
      "\n",
      "The following question is about Star Wars: What is the name of Anakin's stepbrother? The answer to the question is:\n",
      "Luke Skywalker\n",
      "\n",
      "The following question is about Star Wars: Who was Boba Fett's father? The answer to the question is:\n",
      "Luke Skywalker\n",
      "\n",
      "The following question is about Star Wars: Who killed Mace Windu? The answer to the question is:\n",
      "Luke Skywalker\n",
      "\n",
      "The following question is about Star Wars: What is the name of the female member of the Jedi High Council who is of the same species as Yoda? The answer to the question is:\n",
      "Yorga\n",
      "\n",
      "The following question is about Star Wars: What was Galen Erso's nickname for his daughter? The answer to the question is:\n",
      "dookie\n",
      "\n",
      "The following question is about Star Wars: True or False, Boba Fett survived the Sarlacc pit The answer to the question is:\n",
      "True\n",
      "\n",
      "The following question is about Star Wars: How many episodes are in the Skywalker Saga? The answer to the question is:\n",
      "10\n",
      "\n",
      "The following question is about Star Wars: Who is known for saying the line: \"I have spoken\"? The answer to the question is:\n",
      "wilhelm wilhelm iii\n",
      "\n",
      "The following question is about Star Wars: Who played Princess Leia? The answer to the question is:\n",
      "Yoko Ono\n",
      "\n",
      "The following question is about Star Wars: Which actor played Luke Skywalker? The answer to the question is:\n",
      "Luke Skywalker\n",
      "\n",
      "The following question is about Star Wars: Who does Obi-Wan say was the \"Chosen One\"? The answer to the question is:\n",
      "Luke Skywalker\n",
      "\n",
      "The following question is about Star Wars: Who does BB-8 belong to? The answer to the question is:\n",
      "The Empire\n",
      "\n",
      "The following question is about Star Wars: How many children does Darth Vader have? The answer to the question is:\n",
      "three\n",
      "\n",
      "The following question is about Star Wars: Lightsabers are powered by what type of crystal? The answer to the question is:\n",
      "a halogen\n",
      "\n",
      "The following question is about Star Wars: Which actor played Lando Calrissian? The answer to the question is:\n",
      "James Earl Jones\n",
      "\n",
      "The following question is about Star Wars: Where do Rey and BB-8 first meet? The answer to the question is:\n",
      "the Star Wars Episode I\n",
      "\n",
      "The following question is about Star Wars: Who said, “I know what I have to do, but I don't know that I have the strength to do it”? The answer to the question is:\n",
      "wilbur\n",
      "\n",
      "The following question is about Star Wars: According to Luke, confronting what is the destiny of a Jedi? The answer to the question is:\n",
      "a Jedi\n",
      "\n",
      "The following question is about Star Wars: Who is Darth Vader's grandson? The answer to the question is:\n",
      "Darth Vader\n",
      "\n",
      "The following question is about Star Wars: Leia said never underestimate a what? The answer to the question is:\n",
      "saber\n",
      "\n",
      "The following question is about Star Wars: X-wing fighters were used by the Resistance or the First Order? The answer to the question is:\n",
      "Resistance\n",
      "\n",
      "The following question is about Star Wars: Jyn Erso said rebellions are built on what? The answer to the question is:\n",
      "a savagery\n",
      "\n",
      "The following question is about Star Wars: Legend describes what as the hidden world of the Sith? The answer to the question is:\n",
      "a secluded planet\n",
      "\n",
      "The following question is about Star Wars: Who killed Snoke? The answer to the question is:\n",
      "Luke Skywalker\n",
      "\n",
      "The following question is about Star Wars: What was Mando's weapon against Moff Gideon's darksaber? The answer to the question is:\n",
      "a grenade\n",
      "\n",
      "The following question is about Star Wars: Who is the black market droidsmith Poe knows on Kijimi? The answer to the question is:\n",
      "Luke Skywalker\n",
      "\n",
      "The following question is about Star Wars: What compasses lead the way to Exegol? The answer to the question is:\n",
      "a saber\n",
      "\n",
      "The following question is about Star Wars: What was Poe Dameron's old job before becoming a pilot? The answer to the question is:\n",
      "a sailor\n",
      "\n",
      "The following question is about Star Wars: Luke said, \"Confronting ___ is the destiny of a Jedi\"? The answer to the question is:\n",
      "a tyrant\n",
      "\n",
      "The following question is about Star Wars: Who killed Han Solo? The answer to the question is:\n",
      "Luke Skywalker\n",
      "\n",
      "The following question is about Star Wars: C-3PO is mechanically incapable of speaking translations from who? The answer to the question is:\n",
      "the Emperor\n",
      "\n",
      "The following question is about Star Wars: According to Zorii, what can give free passage through any blockade and landing privileges, any vessel? The answer to the question is:\n",
      "a saber\n",
      "\n",
      "The following question is about Star Wars: Who famously said, \"These aren't the droids you're looking for\"? The answer to the question is:\n",
      "wilhelm wilhelm iii\n",
      "\n",
      "The following question is about Star Wars: Rey and Kylo are a what ___ in the Force? The answer to the question is:\n",
      "a savage\n",
      "\n",
      "The following question is about Star Wars: Who wins by making people think they are alone? The answer to the question is:\n",
      "The Empire Strikes Back\n",
      "\n",
      "5 out of 66 correct\n"
     ]
    }
   ],
   "source": [
    "# I'm using this:\n",
    "# Prefix: 'The following is a question inspired by the Star Wars series. The question is:'\n",
    "# Postfix: 'A short and precise answer is:'\n",
    "\n",
    "!python3 qa/qa.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It performs slightly better, but still not great. When I reveal in the pre-fix that it is questions about Star Wars it is obvious that it has a better idea about where in the space of words to be looking, but it is lacking some preciness. It is probably because the question are very specific, and there are to many answers with an close to equal amount of probability. It has simply not been trained enough to be able to distinguish these specific questions. Also there is a big person gallery to remember in Star Wars and the questions contain a lot of names, which makes it harder to predict the right answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We also provide you with the raw text from Wookieepedia, this is a fandom wiki with information about the Star Wars universe written in English. It has been scraped using the procedure described on https://robvanderg.github.io/datasets/wikia/, and is available in starwarsfandomcom-20200223.txt.cleaned.tok.uniq.txt.gz. Use the words from the questions to find the 5 sentences with the highest word overlap (the raw sentences with the largest coverage of words with respect to the question). Add these sentences as a prefix, separated with newlines. Does performance increase?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Bonus: experiment with better variants of data selection, what is the highest score you can obtain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Domain Adaptation through Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Domain adaptation through fine tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
